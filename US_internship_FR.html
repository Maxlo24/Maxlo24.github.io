<!DOCTYPE HTML>
<html>
	<head>
		<title>Machine learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157529848-1"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-157529848-1');
		</script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<a href="index_FR.html" class="logo">portfolio</a>
			</header>

			<!-- Nav -->
			<nav id="nav">
				<ul class="links">
                    <li style="background-color: rgba(39, 39, 39, 0.5);"><a href="US_internship_EN.html">FR/EN</a></li>
                    <li><a href="index_FR.html">Mes projets</a></li>
                    <li><a href="aPropos.html">à propos</a></li>
                    <li><a href="services_FR.html">Services</a></li>
				</ul>
				<ul class="icons">
					<li><a href="https://www.linkedin.com/in/maxime-gillot-6b0920179/" class="icon brands fa-linkedin"><span class="label">Instagram</span></a></li>
					<li><a href="https://github.com/Maxlo24" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
					<li><a href="https://www.instagram.com/maxime_gt69" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
					<li><a href="https://www.youtube.com/@maximeg3178/videos" class="icon brands alt fa-youtube"><span class="label">youtube</span></a></li>
				</ul>
			</nav>
					<div id="BtnHautPage">
						<a href="#main" class="button icon solid solo fa-arrow-up scrolly">Top</a>
					</div>
					


				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">

                                <!-- Introduction  -->

                                <header class="major">
                                    <span class="date">Août 2021 - 2022</span>
                                    <h1>Outils Dentaires Automatisés avec l'Apprentissage Profond</h1>
                                    <p style="text-align: justify;"> 
                                        Pendant un an, j'ai travaillé pour <b>l'école dentaire de l'Université du Michigan</b>, Ann Arbor, aux États-Unis.
                                        Le département d'orthodontie et de dentisterie pédiatrique dispose d'un groupe de recherche composé d'informaticiens et d'orthodontistes.
                                        Le laboratoire collabore avec les Neuro Image Research and Analysis Laboratories (<a href="https://www.med.unc.edu/psych/research/niral/"><b class="Hlink">NIRAL</b></a>) en Caroline du Nord 
                                        ainsi qu'avec <a href="https://www.kitware.com"><b class="Hlink">Kitware, Inc.</b></a>, une entreprise spécialisée dans la recherche et le développement de logiciels open source dans les domaines de la vision par ordinateur, de l'imagerie médicale, de la visualisation, de la publication de données 3D et du développement de logiciels techniques. 
                                        Au cours de ce stage, <b>j'ai développé et mis en œuvre deux outils d'apprentissage automatique</b> (<a href="https://github.com/Maxlo24/ALI_CBCT"><b class="Hlink">ALICBCT</b></a> et <a href="https://github.com/Maxlo24/AMASSS_CBCT"><b class="Hlink">AMASSS</b></a>) pour aider les cliniciens experts dans le diagnostic, le traitement et la recherche sur les scans crânio-faciaux des patients.
                                        Les outils développés sont maintenant déployés sur deux écosystèmes open source disponibles gratuitement pour tous : le <a href="https://smart-doc.dent.umich.edu/#/"><b class="Hlink">Smart-DOC</b></a>, un système basé sur le web, et sur <a href="https://www.slicer.org"><b class="Hlink">3D Slicer</b></a>, un logiciel open source d'analyse d'images et de visualisation scientifique.
                                        Cela a été possible grâce à une collaboration de centres cliniques du monde entier. Grâce à des efforts communs et à une collaboration, nous avons déjà développé quatre outils d'apprentissage automatique prêts à être utilisés sur le module <a href="https://github.com/DCBIA-OrthoLab/SlicerAutomatedDentalTools"><b class="Hlink">Slicer Automated Dental Tools</b></a>.
                                    </p>
                                
                                    <p style="text-align: justify;"> 
                                        <b>J'ai publié un article en tant que premier auteur sur chacun des outils suivants : <a href="#AMASSS" class="scrolly">AMASSS</a> et <a href="#ALICBCT" class="scrolly">ALICBCT</a></b> <br>
                                        Mon profil Google Scholar : <a href="https://scholar.google.com/citations?user=i_tpL0gAAAAJ&hl=en&oi=ao"><b class="Hlink">Maxime Gillot</b></a> <br>
										J'ai également écrit un <a href="#book" class="scrolly">book chapter</a> décrivant le travail de notre équipe. <br>
                                    </p>
                                </header>
                                
                                <h2> <a id="AMASSS"></a> AMASSS</h2>
                                
                                <p style="text-align: justify;">
                                    Publication dans PLOS ONE : <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0275033"><b class="Hlink">Automatic multi-anatomical skull structure segmentation of cone-beam computed tomography scans using 3D UNETR</b></a>
                                </p>
                                
                                <p style="text-align: justify;" >
                                    AMASSS signifie Segmentation Automatique de la Structure Crânienne Multi-Anatomique. Dans ce cas, la segmentation des scans CBCT. La segmentation des images médicales et dentaires est une tâche visuelle visant à identifier les voxels des organes ou des lésions à partir des scans de niveaux de gris de l'arrière-plan.
                                    Cela représente une condition préalable à l'analyse d'images médicales. Particulièrement pour des conditions dentaires et craniofaciales difficiles, telles que les déformations dentofaciales, les anomalies craniofaciales et l'impact des dents, l'analyse quantitative d'images nécessite des solutions efficaces pour résoudre la tâche chronophage et dépendante de l'utilisateur de la segmentation d'images.
                                    <br>
                                    Les images CBCT à grand champ de vision couramment utilisées pour les applications cliniques en orthodontie et en chirurgie maxillofaciale nécessitent en moyenne des segmentation détaillées par des cliniciens expérimentés : 7 heures de travail pour l'ensemble du visage, 1,5 heure pour la mandibule, 2 heures pour le maxillaire, 2 heures pour la base du crâne (CB), 1 heure pour les vertèbres cervicales (CV) et 30 minutes pour la peau.
                                    L'entraînement des modèles d'apprentissage automatique a permis de segmenter les scans CBCT en moins de 5 minutes avec une grande précision.
                                </p>
                                
                                <div  style="box-shadow: none;" class="image main"><img src="images/Machine_Learning/AMASSS.png" alt="Tous les résultats"></div>
                                
                                <h2> <a id="ALICBCT"></a> ALICBCT</h2>
                                
                                <p style="text-align: justify;">
                                    Publication : <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ocr.12642"><b class="Hlink">Identification automatique des repères dans la tomographie par faisceau conique</b></a> 
                                </p>
                                
                                <p style="text-align: justify;">
                                
                                    ALICBCT signifie <b>Identification Automatique des Repères</b> dans les scans CBCT. La localisation précise des repères anatomiques pour les données d'imagerie médicale est un problème difficile en raison de la fréquente ambiguïté de leur apparence et de la grande variabilité des structures anatomiques. 
                                    La détection de repères représente une condition préalable à l'analyse d'images médicales. Elle soutient l'ensemble des processus cliniques, du diagnostic à la planification du traitement, de l'intervention au suivi des changements anatomiques ou des conditions pathologiques, ainsi que des simulations.
                                    Dans ce travail, nous avons présenté une nouvelle méthode inspirée d'une technique d'apprentissage par renforcement profond. 
                                    <b>La tâche de détection de repères est configurée comme un problème de classification du comportement pour un agent artificiel qui se déplace dans la grille de voxels de l'image à différentes résolutions spatiales.</b> 
                                
                                </p>

								<div class="image main"><div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/879178335?autoplay=1&loop=1&title=0&byline=0&portrait=0&autopause=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script></div>

								<p>Présentaton des différents outils au Moyers symposium 2022</p>

                                <div  style="box-shadow: none;"  class="image main"><img src="images/Machine_Learning/ALICBCT.png" alt="Tous les résultats"></div>
                                
								<header class="major">
									<span class="date" id="1">fichier</span>
									<h1><a id="book"></a> Book chapter</h1>
									<embed src="Documents/Book_chapter.pdf" width="900px" height="1150px" />
									<!-- <div class="image main"><img src="" alt="Mon CV" /></div> -->
								</header>

								<div class="image main"><div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/879172223?autoplay=1&loop=1&title=0&byline=0&portrait=0&autopause=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script></div>

								<p style="text-align: justify;">
									L'agent virtuel dispose d'un champ de vision en forme de boîte autour de son point central. Il analyse ce dernier à chaque itération pour déterminer la prochaine direction dans laquelle il doit se déplacer. 
									En répétant ces étapes jusqu'à ce que l'agent cesse de se déplacer, nous atteignons finalement la zone du repère souhaité. Nous effectuons ce processus à deux résolutions différentes :
									 d'abord à une résolution plus basse pour se déplacer rapidement et approximer l'emplacement, puis à une résolution plus élevée pour déterminer précisément l'emplacement du repère.
                                </p>

								<p>Petite vidéo de présentation du projet ALI-IOS :</p>


								<div class="image main"><div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/879174045?autoplay=1&loop=1&title=0&byline=0&portrait=0&autopause=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script></div>


                            </section>

					</div>

				<!-- Footer -->
				<footer id="footer">
						<section class="split contact">
							<header class="major">
								<span class="date" id="contact">contact</span>
								<h1>Me contacter</h1>
							</header>
							<section class="alt">
								<h3>Addresse</h3>
								<p>Lyon, France</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">+33 (0)7 81 17 52 42</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="#">maxime.gillot@cpe.fr</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/maxime-gillot-6b0920179/" class="icon brands fa-linkedin"><span class="label">Instagram</span></a></li>
									<li><a href="https://github.com/Maxlo24" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

					<header id="header">
						<a href="index_FR.html" class="logo">portfolio</a>
					</header>


				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Maxime GILLOT</li><li>CSS & JS: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>